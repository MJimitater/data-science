{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   }
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import git2net\n",
    "\n",
    "#path to copy of virgin db:\n",
    "sqlite_db_file = '/home/luc/pip/git2net/group_work_1/git2net_tutorial (Kopie).db'\n",
    "\n",
    "#set variable to whatever folder the to-be-mined git repository is located:\n",
    "repo_dir = '/home/luc/pip/git2net/ds_gw_1/TLP'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uncomment this block, if mining is wished\n",
    "\n",
    "#check, if database file is there; remove if yes\n",
    "#if os.path.exists(sqlite_db_file):\n",
    "#    os.remove(sqlite_db_file)\n",
    "#    print('sqlite_db_file removed!')\n",
    "\n",
    "#mining process: \n",
    "#repo_dir: to-be-mined git repository\n",
    "#sqlite_db_file: to-be-stored values in database\n",
    "\n",
    "#git2net.mine_git_repo(repo_dir, sqlite_db_file)\n",
    "\n",
    "#git2net.mining_state_summary(repo_dir, sqlite_db_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "#connect to database file:\n",
    "con = sqlite3.connect(sqlite_db_file)\n",
    "c = con.cursor()\n",
    "\n",
    "\n",
    "#create a new table with the desired column names:\n",
    "query0=\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS df2(\n",
    "    hash TEXT PRIMARY KEY,\n",
    "    committer_name TEXT,\n",
    "    committer_date TEXT,\n",
    "    original_commit_deletion TEXT,\n",
    "    levenshtein_dist INTEGER);\n",
    "\"\"\"\n",
    "\n",
    "#select only rows of table \"edits\", where edit_type='replacement', \n",
    "#merge this with table \"commits\" along the key 'hash',\n",
    "#group this by unique hashes, taking the sum of the levenshtein dist of same hashes\n",
    "#finally, insert this into a new table \"df2\" to avoid key words as column names:\n",
    "query1=\"\"\"\n",
    "    INSERT INTO df2(hash, committer_name, committer_date, original_commit_deletion, levenshtein_dist)\n",
    "    SELECT hash, committer_name, committer_date, original_commit_deletion, \"SUM(levenshtein_dist)\"\n",
    "    FROM\n",
    "    (\n",
    "    SELECT \n",
    "    hash,\n",
    "    committer_name,\n",
    "    committer_date,\n",
    "    original_commit_deletion,\n",
    "    SUM(levenshtein_dist)\n",
    "    FROM(SELECT * FROM edits INNER JOIN commits ON commits.hash = edits.commit_hash WHERE edit_type = 'replacement')\n",
    "    GROUP BY hash\n",
    "    );\n",
    "\"\"\"\n",
    "\n",
    "#count the distinct committer names:\n",
    "query1_1=\"\"\"\n",
    "    SELECT count(DISTINCT committer_name) FROM df2;\n",
    "\"\"\"\n",
    "\n",
    "#select the distinct committer names\n",
    "query1_2=\"\"\"\n",
    "    SELECT DISTINCT committer_name FROM df2;\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "#execute the two queries on the database:\n",
    "c.execute(query0)\n",
    "c.execute(query1)\n",
    "c.execute(query1_1)\n",
    "\n",
    "#store amount of distinct committer names as int...\n",
    "number_committers = c.fetchone()[0]\n",
    "\n",
    "#...before executing the next query:\n",
    "c.execute(query1_2)\n",
    "\n",
    "#store names of committers in list:\n",
    "committer_list = c.fetchall()\n",
    "\n",
    "\n",
    "#save the queries:\n",
    "con.commit()\n",
    "\n",
    "#close the database to make it accessible for others:\n",
    "con.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[]\n"
    }
   ],
   "source": [
    "#create new empty table:\n",
    "query01=\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS df3(\n",
    "    hash TEXT PRIMARY KEY,\n",
    "    committer_name TEXT,\n",
    "    committer_date TEXT,\n",
    "    original_commit_deletion TEXT,\n",
    "    levenshtein_dist INTEGER);\n",
    "\"\"\"\n",
    "\n",
    "#sort committer_date ascending:\n",
    "query2=\"\"\"\n",
    "    INSERT INTO df3(hash, committer_name, committer_date, original_commit_deletion, levenshtein_dist)\n",
    "    SELECT *\n",
    "    FROM\n",
    "    (\n",
    "    SELECT * \n",
    "    FROM df2 \n",
    "    ORDER BY committer_date ASC\n",
    "    );\n",
    "\"\"\"\n",
    "\n",
    "#delete table df2:\n",
    "query3=\"\"\"\n",
    "    DROP TABLE df2;\n",
    "\"\"\"\n",
    "\n",
    "#delete table df3:\n",
    "#query4=\"\"\"\n",
    "#    DROP TABLE df3;\n",
    "#\"\"\"\n",
    "\n",
    "\n",
    "#ordering:\n",
    "#query0, query2_edited, query3\n",
    "#then:\n",
    "#query01, query2_edited, query4\n",
    "\n",
    "\n",
    "con = sqlite3.connect(sqlite_db_file)\n",
    "c = con.cursor()\n",
    "\n",
    "#execute the queries:\n",
    "c.execute(query01)\n",
    "c.execute(query2)\n",
    "c.execute(query3)\n",
    "#result: df3\n",
    "\n",
    "print(c.fetchmany(5))\n",
    "\n",
    "con.commit()\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = sqlite3.connect(sqlite_db_file)\n",
    "\n",
    "#output df2 as a pandas dataframe\n",
    "df = pd.read_sql_query(\"SELECT * FROM df3;\", con=con)\n",
    "\n",
    "#turn Pandas dataframe into numpy array:\n",
    "df = df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#function for adding a full-length '0'-column to the dataframe:\n",
    "def add_column(df):\n",
    "    new_column = np.array(np.zeros(df.shape[0]))\n",
    "    return np.column_stack((df, new_column))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add column:\n",
    "df = add_column(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "2010-01-22 18:16:44\n"
    }
   ],
   "source": [
    "#make date stamps readible, by turning them into a datetime-object:\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "def date_to_int(date_str):\n",
    "    return datetime.strptime(date_str, '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "print(date_to_int(df[0][2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate amount of time (in seconds) since last commit for every authors commit; save values in the newly created column\n",
    "\n",
    "for i in range(df.shape[0]-1):\n",
    "    #grab the author and his commit_time:\n",
    "    author = df[i][1]\n",
    "    commit_time = df[i][2]\n",
    "    for j in range(i+1,df.shape[0]):\n",
    "        #search for the his next commit:\n",
    "        author_next = df[j][1]\n",
    "        commit_time_next = df[j][2]\n",
    "        #if there is a \"next commit\", calculate the commit_time differences (in total seconds) and save the value in the last column:\n",
    "        if author == author_next:\n",
    "                df[j][5] = (date_to_int(commit_time_next) - date_to_int(commit_time)).total_seconds()\n",
    "                break\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "(942, 7)\n"
    }
   ],
   "source": [
    "#add empty 6th column:\n",
    "df = add_column(df)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate a measure for productivity:\n",
    "#levenshtein_dist / time_between_commits = levenshtein_dist per second\n",
    "\n",
    "for i in range(df.shape[0]):\n",
    "        #if time_between_commits = 0, then set productivity to 0\n",
    "    if df[i][5] == 0:\n",
    "        df[i][6] = 0\n",
    "        #else, calculate productivity measure and store in 6th column\n",
    "    else:     \n",
    "        df[i][6] = df[i][4] / df[i][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add empty 7th column:\n",
    "df = add_column(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find out, if author collaborated or not:\n",
    "#'1' if his parents hash is another authors commit hash,\n",
    "#'0' if not\n",
    "#add this boolean in a separate column:\n",
    "\n",
    "for i in range(df.shape[0]):\n",
    "    parent_hash = df[i][3]\n",
    "    #set orig_author to default 'nobody'\n",
    "    #(important for genesis commit hashes)\n",
    "    orig_author = 'nobody'\n",
    "    for j in range(i):\n",
    "        #check whether any other prior hash is from the same author\n",
    "        if df[j][0] == parent_hash:\n",
    "            #update orig_author with real author name:\n",
    "            orig_author = df[j][1]\n",
    "            #if original hash found, then break:\n",
    "            break\n",
    "    #set a place holder in the 7th column:\n",
    "    df[i][7] = 'no parent_hash'\n",
    "    #if author collaborated, save '1', if not, save '0':\n",
    "    #only iff orig_author was another author, did the author collaborate:\n",
    "    if orig_author != df[i][1] and orig_author != 'nobody':\n",
    "        df[i][7] = 1\n",
    "    else:\n",
    "        df[i][7] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete commit_hash:\n",
    "df = np.delete(df,0,1)\n",
    "#delete commit_time:\n",
    "df = np.delete(df,1,1)\n",
    "#delete parent_hash:\n",
    "df = np.delete(df,1,1)\n",
    "#delete levensthein_dist:\n",
    "df = np.delete(df,1,1)\n",
    "#delete time_between_commits:\n",
    "df = np.delete(df,1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "(942, 3)\n"
    }
   ],
   "source": [
    "#turn numpy array into pandas dataframe:\n",
    "#insert first column of df as 'committer_names':\n",
    "pdf = pd.DataFrame({'committer_names': df[:,0]})\n",
    "#insert second column of df as 'productivity_per_second':\n",
    "pdf['productivity_per_second'] = df[:,1]\n",
    "#insert third column of df as 'collab_bool':\n",
    "pdf['collab_bool'] = df[:,2]\n",
    "\n",
    "print(pdf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf['productivity_per_second'] = pd.to_numeric(pdf['productivity_per_second'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "OLS Regression Results                              \n===================================================================================\nDep. Variable:     productivity_per_second   R-squared:                       0.008\nModel:                                 OLS   Adj. R-squared:                 -0.004\nMethod:                      Least Squares   F-statistic:                    0.6453\nDate:                     Tue, 12 Nov 2019   Prob (F-statistic):              0.790\nTime:                             22:28:43   Log-Likelihood:                -2825.6\nNo. Observations:                      942   AIC:                             5675.\nDf Residuals:                          930   BIC:                             5733.\nDf Model:                               11                                         \nCovariance Type:                 nonrobust                                         \n=======================================================================================================\n                                          coef    std err          t      P>|t|      [0.025      0.975]\n-------------------------------------------------------------------------------------------------------\nIntercept                            8.236e-06      3.457   2.38e-06      1.000      -6.785       6.785\ncollab_bool[T.1]                       -1.0798      1.486     -0.727      0.468      -3.995       1.836\ncommitter_names[T.Andr√© Erdmann]        9.6649      4.945      1.954      0.051      -0.040      19.370\ncommitter_names[T.Connor Prussin]       1.0798      6.169      0.175      0.861     -11.028      13.187\ncommitter_names[T.GitHub]               0.8098      4.378      0.185      0.853      -7.783       9.402\ncommitter_names[T.Kai-Heng Feng]        1.0798      6.169      0.175      0.861     -11.028      13.187\ncommitter_names[T.Maxime Gauduin]       1.2096      5.110      0.237      0.813      -8.819      11.238\ncommitter_names[T.Qiang Yu]             1.0798      6.169      0.175      0.861     -11.028      13.187\ncommitter_names[T.SammysHP]             1.0798      6.169      0.175      0.861     -11.028      13.187\ncommitter_names[T.TK]                   0.7245      4.572      0.158      0.874      -8.248       9.696\ncommitter_names[T.Thomas Koch]          0.3041      3.461      0.088      0.930      -6.488       7.096\ncommitter_names[T.Timofey Titovets]     1.0798      6.169      0.175      0.861     -11.028      13.187\n==============================================================================\nOmnibus:                     2354.421   Durbin-Watson:                   1.337\nProb(Omnibus):                  0.000   Jarque-Bera (JB):         18144767.642\nSkew:                          25.023   Prob(JB):                         0.00\nKurtosis:                     681.073   Cond. No.                         112.\n==============================================================================\n\nWarnings:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
    }
   ],
   "source": [
    "model = sm.ols(formula='productivity_per_second ~ collab_bool + committer_names', data = pdf)\n",
    "fit = model.fit()\n",
    "\n",
    "print(fit.summary())"
   ]
  }
 ]
}