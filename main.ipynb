{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   }
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "<module 'posixpath' from '/home/luc/pip/git2net/pip_ds/lib/python3.7/posixpath.py'>\n"
    }
   ],
   "source": [
    "import os\n",
    "import git2net\n",
    "\n",
    "print(os.path)\n",
    "\n",
    "#path to copy of virgin db:\n",
    "sqlite_db_file = '/home/luc/pip/git2net/group_work_1/git2net_tutorial (Kopie).db'\n",
    "\n",
    "\n",
    "#set variable to whatever folder the to-be-mined git repository is located:\n",
    "repo_dir = '/home/luc/pip/git2net/ds_gw_1/TLP'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uncomment this block, if mining is wished\n",
    "\n",
    "#check, if database file is there; remove if yes\n",
    "#if os.path.exists(sqlite_db_file):\n",
    "#    os.remove(sqlite_db_file)\n",
    "#    print('sqlite_db_file removed!')\n",
    "\n",
    "#mining process: \n",
    "#repo_dir: to-be-mined git repository\n",
    "#sqlite_db_file: to-be-stored values in database\n",
    "\n",
    "#git2net.mine_git_repo(repo_dir, sqlite_db_file)\n",
    "\n",
    "#git2net.mining_state_summary(repo_dir, sqlite_db_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "#connect to database file:\n",
    "con = sqlite3.connect(sqlite_db_file)\n",
    "c = con.cursor()\n",
    "\n",
    "\n",
    "#create a new table:\n",
    "query0=\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS df2(\n",
    "    hash TEXT PRIMARY KEY,\n",
    "    committer_name TEXT,\n",
    "    committer_date TEXT,\n",
    "    original_commit_deletion TEXT,\n",
    "    levenshtein_dist INTEGER);\n",
    "\"\"\"\n",
    "\n",
    "#select only rows of table \"edits\", where edit_type='replacement', \n",
    "#merge this with table \"commits\" along the key 'hash',\n",
    "#group this by unique hashes, taking the sum of the levenshtein dist of same hashes\n",
    "#finally, insert this into a new table \"df2\" to avoid key words as column names:\n",
    "query1=\"\"\"\n",
    "    INSERT INTO df2(hash, committer_name, committer_date, original_commit_deletion, levenshtein_dist)\n",
    "    SELECT hash, committer_name, committer_date, original_commit_deletion, \"SUM(levenshtein_dist)\"\n",
    "    FROM\n",
    "    (\n",
    "    SELECT \n",
    "    hash,\n",
    "    committer_name,\n",
    "    committer_date,\n",
    "    original_commit_deletion,\n",
    "    SUM(levenshtein_dist)\n",
    "    FROM(SELECT * FROM edits INNER JOIN commits ON commits.hash = edits.commit_hash WHERE edit_type = 'replacement')\n",
    "    GROUP BY hash\n",
    "    );\n",
    "\"\"\"\n",
    "\n",
    "query1_1=\"\"\"\n",
    "    SELECT count(DISTINCT committer_name) FROM df2;\n",
    "\"\"\"\n",
    "\n",
    "query1_2=\"\"\"\n",
    "    SELECT DISTINCT committer_name FROM df2;\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "#execute the two queries on the database:\n",
    "c.execute(query0)\n",
    "c.execute(query1)\n",
    "c.execute(query1_1)\n",
    "\n",
    "#store amount of distinct committer names as int...\n",
    "number_committers = c.fetchone()[0]\n",
    "\n",
    "#...before executing the next query:\n",
    "c.execute(query1_2)\n",
    "\n",
    "#store names of committers in list:\n",
    "committer_list = c.fetchall()\n",
    "\n",
    "\n",
    "#save the queries:\n",
    "con.commit()\n",
    "\n",
    "#close the database to make it accessible for others:\n",
    "con.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[]\n"
    }
   ],
   "source": [
    "query01=\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS df3(\n",
    "    hash TEXT PRIMARY KEY,\n",
    "    committer_name TEXT,\n",
    "    committer_date TEXT,\n",
    "    original_commit_deletion TEXT,\n",
    "    levenshtein_dist INTEGER);\n",
    "\"\"\"\n",
    "\n",
    "#sort committer_date ascending:\n",
    "\n",
    "query2=\"\"\"\n",
    "    INSERT INTO df3(hash, committer_name, committer_date, original_commit_deletion, levenshtein_dist)\n",
    "    SELECT *\n",
    "    FROM\n",
    "    (\n",
    "    SELECT * \n",
    "    FROM df2 \n",
    "    ORDER BY committer_date ASC\n",
    "    );\n",
    "\"\"\"\n",
    "\n",
    "query3=\"\"\"\n",
    "    DROP TABLE df2;\n",
    "\"\"\"\n",
    "\n",
    "query4=\"\"\"\n",
    "    DROP TABLE df3;\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "#ordering:\n",
    "#query0, query2_edited, query3\n",
    "#then:\n",
    "#query01, query2_edited, query4\n",
    "\n",
    "\n",
    "con = sqlite3.connect(sqlite_db_file)\n",
    "c = con.cursor()\n",
    "\n",
    "c.execute(query01)\n",
    "c.execute(query2)\n",
    "c.execute(query3)\n",
    "#result: df3\n",
    "\n",
    "print(c.fetchmany(5))\n",
    "\n",
    "con.commit()\n",
    "con.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adding_up(df):\n",
    "    print(df)\n",
    "    print(df.shape[0])\n",
    "    if df.shape[0] == 1:\n",
    "        return df\n",
    "    for i in range(df.shape[0]):\n",
    "        print('i=',i)        \n",
    "        for j in range(i+1, df.shape[0]):\n",
    "            if df.iloc[i]['A'] < df.iloc[j]['A']:\n",
    "                print(df.iloc[i]['D'])\n",
    "                print(df.iloc[j]['D'])\n",
    "                tmp = df.iloc[i]['D'] + df.iloc[j]['D']\n",
    "                df.iloc[i]['D'] = tmp\n",
    "                print(df.iloc[i]['D'])\n",
    "                print('Es wird gedropped, anschlieÃŸend Indexreset:\\n')\n",
    "                df = df.drop(df.index[j])        \n",
    "                df = df.reset_index(drop=True)\n",
    "                print(df)\n",
    "                #print(type(df))\n",
    "                return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = sqlite3.connect(sqlite_db_file)\n",
    "\n",
    "#output df2 as a pandas dataframe\n",
    "df = pd.read_sql_query(\"SELECT * FROM df3;\", con=con)\n",
    "\n",
    "#print(df.head(3))\n",
    "#print(df.shape)\n",
    "#print(type(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#turn Pandas dataframe into numpy array:\n",
    "df = df.values\n",
    "\n",
    "#print(type(df))\n",
    "#print(df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def add_column(df):\n",
    "    #add a full-length '0'-column to the dataframe:\n",
    "    new_column = np.array(np.zeros(df.shape[0]))\n",
    "    return np.column_stack((df, new_column))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = add_column(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make date stamps readible, by turning them into a datetime-object:\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "def date_to_int(date_str):\n",
    "    return datetime.strptime(date_str, '%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "2010-01-22 18:16:44\n"
    }
   ],
   "source": [
    "print(date_to_int(df[0][2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate amount of time (in seconds) since last commit for every authors commit; save values in separate column\n",
    "\n",
    "for i in range(df.shape[0]-1):\n",
    "    author = df[i][1]\n",
    "    commit_time = df[i][2]\n",
    "    for j in range(i+1,df.shape[0]):\n",
    "        author_next = df[j][1]\n",
    "        commit_time_next = df[j][2]\n",
    "        if author == author_next:\n",
    "                df[j][5] = (date_to_int(commit_time_next) - date_to_int(commit_time)).total_seconds()\n",
    "                break\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "(942, 7)\n"
    }
   ],
   "source": [
    "df = add_column(df)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate a measure for productivity:\n",
    "#levenshtein_dist / time_between_commits = levenshtein_dist per second\n",
    "\n",
    "for i in range(df.shape[0]):\n",
    "    if df[i][5] == 0:\n",
    "        df[i][6] = 0\n",
    "    else:     \n",
    "        df[i][6] = df[i][4] / df[i][5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = add_column(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find out, if author collaborated or not:\n",
    "#'1' if his parents hash is another authors commit hash,\n",
    "#'0' if not\n",
    "\n",
    "for i in range(df.shape[0]):\n",
    "    parent_hash = df[i][3]\n",
    "    orig_author = 'nobody'\n",
    "    for j in range(i):\n",
    "        if df[j][0] == parent_hash:\n",
    "            orig_author = df[j][1]\n",
    "            #if original hash found, then break:\n",
    "            break\n",
    "    df[i][7] = 'no parent_hash'\n",
    "    #if author collaborated, save '1', if not, save '0':\n",
    "    if orig_author != df[i][1] and orig_author != 'nobody':\n",
    "        df[i][7] = 1\n",
    "    else:\n",
    "        df[i][7] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "(942, 19)\n"
    }
   ],
   "source": [
    "#insert as many additional columns into numpy array as there are committer names:\n",
    "orig_shape_df = df.shape[1]\n",
    "for i in range(len(committer_list)):\n",
    "    df = add_column(df)\n",
    "print(df.shape)\n",
    "\n",
    "#write dummy variables into the corresponding column:\n",
    "for i in range(len(committer_list)):\n",
    "    for j in range(df.shape[0]):\n",
    "        if df[j][1] == committer_list[i][0]:\n",
    "            df[j][i + orig_shape_df] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete all the other columns that are not needed for the regression analysis:\n",
    "df = np.delete(df,0,1)\n",
    "df = np.delete(df,0,1)\n",
    "df = np.delete(df,0,1)\n",
    "df = np.delete(df,0,1)\n",
    "df = np.delete(df,0,1)\n",
    "df = np.delete(df,0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "(942, 13)\n"
    }
   ],
   "source": [
    "#turn numpy array into pandas dataframe:\n",
    "\n",
    "#this is done manually, perhaps automate this?!\n",
    "pdf = pd.DataFrame({'productivity_per_second': df[:,0]})\n",
    "pdf = pdf.assign(collab_bool = df[:,1])\n",
    "pdf = pdf.assign(thomas_koch = df[:,2])\n",
    "pdf = pdf.assign(github = df[:,3])\n",
    "pdf = pdf.assign(sammys_hp = df[:,4])\n",
    "pdf = pdf.assign(andre_erdmann = df[:,5])\n",
    "pdf = pdf.assign(aaditya_bagga = df[:,6])\n",
    "pdf = pdf.assign(timofey_titovets = df[:,7])\n",
    "pdf = pdf.assign(connor_prussin = df[:,8])\n",
    "pdf = pdf.assign(tk = df[:,9])\n",
    "pdf = pdf.assign(kai_heng_feng = df[:,10])\n",
    "pdf = pdf.assign(maxime_gauduin = df[:,11])\n",
    "pdf = pdf.assign(qiang_yu= df[:,12])\n",
    "\n",
    "print(pdf.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[-3.79310801e+12]\n[[-1.07474422e+00  3.79310801e+12  3.79310801e+12  3.79310801e+12\n   3.79310801e+12  3.79310801e+12  3.79310801e+12  3.79310801e+12\n   3.79310801e+12  3.79310801e+12  3.79310801e+12  3.79310801e+12]]\n"
    }
   ],
   "source": [
    "#perform multiple linear regression:\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "mlr = LinearRegression()\n",
    "\n",
    "mlr.fit(pdf[['collab_bool', 'thomas_koch', 'github', 'sammys_hp', 'andre_erdmann', 'aaditya_bagga', 'timofey_titovets', 'connor_prussin', 'tk', 'kai_heng_feng', 'maxime_gauduin', 'qiang_yu']], pdf[['productivity_per_second']])\n",
    "\n",
    "print(mlr.intercept_)\n",
    "print(mlr.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.30371094]])"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#try a prediction:\n",
    "#what is the productivity_per_second for thomas_koch, who is not collaborating? :\n",
    "mlr.predict([[0,1,0,0,0,0,0,0,0,0,0,0]])"
   ]
  }
 ]
}