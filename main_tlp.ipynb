{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   }
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring the Ringelmann effect with git2net\n",
    "**Lucas Schelkes**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In this project, we analyze the effect of group size on the productivity in GitHub projects. \n",
    "\n",
    "Using the tool `git2net`, we will analyse the following GitHub repositories:\n",
    "\n",
    "1. `TLP` (power saving tool for ThinkPads) as a small example (11 committers)\n",
    "\n",
    "2. `Android` (Android architecture blueprints v2 for building apps) as an medium-sized example (40 committers)\n",
    "\n",
    "3. `nginx-proxy` (sets up container for running nginx) as an medium-large-sized example (60 committers)\n",
    "\n",
    "4. `hello-world` (\"hello world\" code in all languages) as a large-sized example (79 committers)\n",
    "\n",
    "Finally we will derive to an interpretation of the analysis in respect to the Ringelmann effect."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## `TLP` repo\n",
    "\n",
    "We first mine the co-editing data from variously sized git repositories.\n",
    "First, we specify the `sqlite` database location and the to-be-mined cloned `GitHub` repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import git2net\n",
    "\n",
    "#path to copy of virgin db:\n",
    "sqlite_db_file = '/home/luc/pip/git2net/group_work_1/tlp (Kopie).db'\n",
    "#sqlite_db_file = '/home/luc/pip/git2net/group_work_1/bitcoin.db'\n",
    "#sqlite_db_file = '/home/luc/pip/git2net/group_work_1/architecture-samples (Kopie).db'\n",
    "\n",
    "\n",
    "\n",
    "#set variable to whatever folder the to-be-mined git repository is located:\n",
    "#repo_dir = '/home/luc/pip/git2net/ds_gw_1/TLP'\n",
    "#repo_dir = '/home/luc/pip/git2net/group_work_1/architecture-samples'"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Mining\n",
    "\n",
    "Only do the mining, if a copy of the \"virgin\" sqlite database has not being saved in `sqlite_db_file`.\n",
    "This copy needs to be freshly recopied from the virgin file in every Notebook run-through to avoid database errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uncomment this block, if mining is wished\n",
    "\n",
    "#check, if database file is there; remove if yes\n",
    "#if os.path.exists(sqlite_db_file):\n",
    "#    os.remove(sqlite_db_file)\n",
    "#    print('sqlite_db_file removed!')\n",
    "\n",
    "#mining process: \n",
    "#repo_dir: to-be-mined git repository\n",
    "#sqlite_db_file: to-be-stored values in database\n",
    "\n",
    "#git2net.mine_git_repo(repo_dir, sqlite_db_file)\n",
    "\n",
    "#git2net.mining_state_summary(repo_dir, sqlite_db_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualization\n",
    "\n",
    "In order to get a first idea of who about did roughly what with whom else, lets look at a graph showing edges from `committer_A` to `committer_B` whenever `committer_A` changed a line of the original author `committer_B`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<style>\n    text.label_qNQqmuAW {\n        text-anchor: middle;\n        font-size: 8px;\n        font-family: Arial, Helvetica, sans-serif;\n        fill: #999999;\n        stroke: #ffffff;\n        stroke-width: 0.1px;\n        opacity: 1.0;\n    }\n    .links_qNQqmuAW line {\n    stroke-opacity: 1.0;\n    }\n    .arrows svg:path {\n        stroke: #666;\n    }\n    .nodes_qNQqmuAW circle {\n    stroke: #fff;\n    stroke-width: 0.5px;\n    }\n</style>\n\n<svg width=\"400\" height=\"400\" id=\"qNQqmuAW\">        \n    <text x=\"20\" y=\"20\" font-family=\"sans-serif\" font-size=\"14px\" fill=\"#666\" style=\"cursor: pointer\"\n        id='qNQqmuAW_svg_txt'>[save svg]</text>\n</svg>\n\n<script charset=\"utf-8\" src=\"https://d3js.org/d3.v4.min.js\"></script>\n<script charset=\"utf-8\">\n\n// Load via requireJS if available (jupyter notebook environment)\ntry {\n    // Problem: require.config will raise an exception when called for the second time \n    require.config({\n        paths: {\n            //d3: \"https://d3js.org/d3.v4.min\"\n            d3: \"https://d3js.org/d3.v4.min.js\".replace(\".js\", \"\")\n        }\n    });\n    console.log(\"Detected requireJS\");\n}\ncatch(err){\n    // a reference error indicates that requireJS does not exist. \n    // other errors may occur due to multiple calls to config\n    if (err instanceof ReferenceError){\n        console.log(\"Detected no requireJS\");\n\n        // Helper function that waits for d3js to be loaded\n        require = function require(symbols, callback) {\n            var ms = 5;\n            window.setTimeout(function(t) {\n                if (window[symbols[0]])\n                    callback(window[symbols[0]]);\n                else \n                    window.setTimeout(arguments.callee, ms);\n            }, ms);\n        }\n    }\n}\n\n// wait until d3js has been loaded\nrequire([\"d3\"], function(d3) {\n    var svg = d3.select(\"#\"+\"qNQqmuAW\"), radius = 6, width = +svg.attr(\"width\"), height = +svg.attr(\"height\");\n\n    var color = d3.scaleOrdinal(d3.schemeCategory20);\n\n    var graph = {\"links\": [{\"source\": \"Thomas Koch\", \"target\": \"TK\", \"color\": \"#999999\", \"width\": 0.5, \"weight\": 0.6551724137931034}, {\"source\": \"TK\", \"target\": \"Thomas Koch\", \"color\": \"#999999\", \"width\": 0.5, \"weight\": 0.3448275862068966}, {\"source\": \"Andr\\u00e9 Erdmann\", \"target\": \"Thomas Koch\", \"color\": \"#999999\", \"width\": 0.5, \"weight\": 0.46875}, {\"source\": \"Thomas Koch\", \"target\": \"Andr\\u00e9 Erdmann\", \"color\": \"#999999\", \"width\": 0.5, \"weight\": 0.49375}, {\"source\": \"Connor Prussin\", \"target\": \"Thomas Koch\", \"color\": \"#999999\", \"width\": 0.5, \"weight\": 0.5}, {\"source\": \"linrunner\", \"target\": \"Thomas Koch\", \"color\": \"#999999\", \"width\": 0.5, \"weight\": 0.9513888888888888}, {\"source\": \"Thomas Koch\", \"target\": \"Connor Prussin\", \"color\": \"#999999\", \"width\": 0.5, \"weight\": 0.5}, {\"source\": \"Timofey\", \"target\": \"Thomas Koch\", \"color\": \"#999999\", \"width\": 0.5, \"weight\": 0.5}, {\"source\": \"Todd Partridge\", \"target\": \"Thomas Koch\", \"color\": \"#999999\", \"width\": 0.5, \"weight\": 0.5}, {\"source\": \"Thomas Koch\", \"target\": \"Pali Roh\\u00e1r\", \"color\": \"#999999\", \"width\": 0.5, \"weight\": 0.865979381443299}, {\"source\": \"Pali Roh\\u00e1r\", \"target\": \"Thomas Koch\", \"color\": \"#999999\", \"width\": 0.5, \"weight\": 0.13402061855670103}, {\"source\": \"Thomas Koch\", \"target\": \"Todd Partridge\", \"color\": \"#999999\", \"width\": 0.5, \"weight\": 0.5}, {\"source\": \"SammysHP\", \"target\": \"Thomas Koch\", \"color\": \"#999999\", \"width\": 0.5, \"weight\": 1.0}, {\"source\": \"Thomas Koch\", \"target\": \"Sascha Riemer\", \"color\": \"#999999\", \"width\": 0.5, \"weight\": 0.6666666666666666}, {\"source\": \"Maxime Gauduin\", \"target\": \"Thomas Koch\", \"color\": \"#999999\", \"width\": 0.5, \"weight\": 0.375}, {\"source\": \"Thomas Koch\", \"target\": \"Maxime Gauduin\", \"color\": \"#999999\", \"width\": 0.5, \"weight\": 0.5}, {\"source\": \"twistedfall\", \"target\": \"Thomas Koch\", \"color\": \"#999999\", \"width\": 0.5, \"weight\": 0.3333333333333333}, {\"source\": \"Maxime Gauduin\", \"target\": \"Sascha Riemer\", \"color\": \"#999999\", \"width\": 0.5, \"weight\": 0.3333333333333333}, {\"source\": \"Thomas Koch\", \"target\": \"Timofey\", \"color\": \"#999999\", \"width\": 0.5, \"weight\": 0.5}, {\"source\": \"Thomas Koch\", \"target\": \"twistedfall\", \"color\": \"#999999\", \"width\": 0.5, \"weight\": 0.6666666666666666}, {\"source\": \"Michel Weimerskirch\", \"target\": \"Thomas Koch\", \"color\": \"#999999\", \"width\": 0.5, \"weight\": 1.0}, {\"source\": \"Andr\\u00e9 Erdmann\", \"target\": \"Maxime Gauduin\", \"color\": \"#999999\", \"width\": 0.5, \"weight\": 0.0625}, {\"source\": \"Stefan Weil\", \"target\": \"Thomas Koch\", \"color\": \"#999999\", \"width\": 0.5, \"weight\": 0.6666666666666666}, {\"source\": \"Seth Wright\", \"target\": \"Thomas Koch\", \"color\": \"#999999\", \"width\": 0.5, \"weight\": 0.2727272727272727}, {\"source\": \"Timofey Titovets\", \"target\": \"Thomas Koch\", \"color\": \"#999999\", \"width\": 0.5, \"weight\": 1.0}, {\"source\": \"Thomas Koch\", \"target\": \"Seth Wright\", \"color\": \"#999999\", \"width\": 0.5, \"weight\": 0.7272727272727273}, {\"source\": \"Qiang Yu\", \"target\": \"Thomas Koch\", \"color\": \"#999999\", \"width\": 0.5, \"weight\": 0.5}, {\"source\": \"Thomas Koch\", \"target\": \"FadeMind\", \"color\": \"#999999\", \"width\": 0.5, \"weight\": 0.5}, {\"source\": \"Thomas Koch\", \"target\": \"Kai-Heng Feng\", \"color\": \"#999999\", \"width\": 0.5, \"weight\": 0.8}, {\"source\": \"linrunner\", \"target\": \"Andr\\u00e9 Erdmann\", \"color\": \"#999999\", \"width\": 0.5, \"weight\": 0.020833333333333332}, {\"source\": \"Thomas Koch\", \"target\": \"Qiang Yu\", \"color\": \"#999999\", \"width\": 0.5, \"weight\": 0.5}, {\"source\": \"Kai-Heng Feng\", \"target\": \"Thomas Koch\", \"color\": \"#999999\", \"width\": 0.5, \"weight\": 0.2}, {\"source\": \"Thomas Koch\", \"target\": \"Rimas Kudelis\", \"color\": \"#999999\", \"width\": 0.5, \"weight\": 1.0}, {\"source\": \"RX14\", \"target\": \"Thomas Koch\", \"color\": \"#999999\", \"width\": 0.5, \"weight\": 1.0}, {\"source\": \"Ottaviocr\", \"target\": \"Thomas Koch\", \"color\": \"#999999\", \"width\": 0.5, \"weight\": 0.6}, {\"source\": \"Thomas Koch\", \"target\": \"Ottaviocr\", \"color\": \"#999999\", \"width\": 0.5, \"weight\": 0.4}, {\"source\": \"FadeMind\", \"target\": \"Thomas Koch\", \"color\": \"#999999\", \"width\": 0.5, \"weight\": 0.5}, {\"source\": \"Johannes Baiter\", \"target\": \"Thomas Koch\", \"color\": \"#999999\", \"width\": 0.5, \"weight\": 0.5}, {\"source\": \"Thomas Koch\", \"target\": \"Johannes Baiter\", \"color\": \"#999999\", \"width\": 0.5, \"weight\": 0.5}, {\"source\": \"Thomas Koch\", \"target\": \"John Salatas\", \"color\": \"#999999\", \"width\": 0.5, \"weight\": 1.0}, {\"source\": \"Thomas Koch\", \"target\": \"Rapha\\u00ebl Halimi\", \"color\": \"#999999\", \"width\": 0.5, \"weight\": 1.0}, {\"source\": \"Thomas Koch\", \"target\": \"linrunner\", \"color\": \"#999999\", \"width\": 0.5, \"weight\": 0.020833333333333332}, {\"source\": \"mahlzahn\", \"target\": \"Thomas Koch\", \"color\": \"#999999\", \"width\": 0.5, \"weight\": 1.0}, {\"source\": \"Aaditya Bagga\", \"target\": \"Maxime Gauduin\", \"color\": \"#999999\", \"width\": 0.5, \"weight\": 1.0}, {\"source\": \"linrunner\", \"target\": \"Maxime Gauduin\", \"color\": \"#999999\", \"width\": 0.5, \"weight\": 0.020833333333333332}, {\"source\": \"Thomas Koch\", \"target\": \"Stefan Weil\", \"color\": \"#999999\", \"width\": 0.5, \"weight\": 0.3333333333333333}], \"nodes\": [{\"id\": \"Thomas Koch\", \"text\": \"Thomas Koch\", \"color\": \"#99ccff\", \"size\": 5.0}, {\"id\": \"TK\", \"text\": \"TK\", \"color\": \"#99ccff\", \"size\": 5.0}, {\"id\": \"Andr\\u00e9 Erdmann\", \"text\": \"Andr\\u00e9 Erdmann\", \"color\": \"#99ccff\", \"size\": 5.0}, {\"id\": \"Connor Prussin\", \"text\": \"Connor Prussin\", \"color\": \"#99ccff\", \"size\": 5.0}, {\"id\": \"linrunner\", \"text\": \"linrunner\", \"color\": \"#99ccff\", \"size\": 5.0}, {\"id\": \"Timofey\", \"text\": \"Timofey\", \"color\": \"#99ccff\", \"size\": 5.0}, {\"id\": \"Todd Partridge\", \"text\": \"Todd Partridge\", \"color\": \"#99ccff\", \"size\": 5.0}, {\"id\": \"Pali Roh\\u00e1r\", \"text\": \"Pali Roh\\u00e1r\", \"color\": \"#99ccff\", \"size\": 5.0}, {\"id\": \"SammysHP\", \"text\": \"SammysHP\", \"color\": \"#99ccff\", \"size\": 5.0}, {\"id\": \"Sascha Riemer\", \"text\": \"Sascha Riemer\", \"color\": \"#99ccff\", \"size\": 5.0}, {\"id\": \"Maxime Gauduin\", \"text\": \"Maxime Gauduin\", \"color\": \"#99ccff\", \"size\": 5.0}, {\"id\": \"twistedfall\", \"text\": \"twistedfall\", \"color\": \"#99ccff\", \"size\": 5.0}, {\"id\": \"Michel Weimerskirch\", \"text\": \"Michel Weimerskirch\", \"color\": \"#99ccff\", \"size\": 5.0}, {\"id\": \"Stefan Weil\", \"text\": \"Stefan Weil\", \"color\": \"#99ccff\", \"size\": 5.0}, {\"id\": \"Seth Wright\", \"text\": \"Seth Wright\", \"color\": \"#99ccff\", \"size\": 5.0}, {\"id\": \"Timofey Titovets\", \"text\": \"Timofey Titovets\", \"color\": \"#99ccff\", \"size\": 5.0}, {\"id\": \"Qiang Yu\", \"text\": \"Qiang Yu\", \"color\": \"#99ccff\", \"size\": 5.0}, {\"id\": \"FadeMind\", \"text\": \"FadeMind\", \"color\": \"#99ccff\", \"size\": 5.0}, {\"id\": \"Kai-Heng Feng\", \"text\": \"Kai-Heng Feng\", \"color\": \"#99ccff\", \"size\": 5.0}, {\"id\": \"Rimas Kudelis\", \"text\": \"Rimas Kudelis\", \"color\": \"#99ccff\", \"size\": 5.0}, {\"id\": \"RX14\", \"text\": \"RX14\", \"color\": \"#99ccff\", \"size\": 5.0}, {\"id\": \"Ottaviocr\", \"text\": \"Ottaviocr\", \"color\": \"#99ccff\", \"size\": 5.0}, {\"id\": \"Johannes Baiter\", \"text\": \"Johannes Baiter\", \"color\": \"#99ccff\", \"size\": 5.0}, {\"id\": \"John Salatas\", \"text\": \"John Salatas\", \"color\": \"#99ccff\", \"size\": 5.0}, {\"id\": \"Rapha\\u00ebl Halimi\", \"text\": \"Rapha\\u00ebl Halimi\", \"color\": \"#99ccff\", \"size\": 5.0}, {\"id\": \"mahlzahn\", \"text\": \"mahlzahn\", \"color\": \"#99ccff\", \"size\": 5.0}, {\"id\": \"Aaditya Bagga\", \"text\": \"Aaditya Bagga\", \"color\": \"#99ccff\", \"size\": 5.0}]};\n    var directed = true;\n\n    var simulation = d3.forceSimulation()        \n       .force(\"link\", d3.forceLink().id(function(d) { return d.id; }).strength(function(d){return d.weight;}))\n        .force(\"charge\", d3.forceManyBody().strength(-20).distanceMax(400))\n       .force(\"repelForce\", d3.forceManyBody().strength(-200).distanceMax(100))\n       .force(\"center\", d3.forceCenter(400 / 2, 400 / 2))\n       .alphaTarget(0.0);\n\n    svg.append(\"defs\").selectAll(\"marker\")\n        .data([\"end\"])\n    .enter().append(\"marker\")\n        .attr(\"id\", String)\n        .attr(\"viewBox\", \"0 -5 20 20\")\n        .attr(\"refX\", 34)\n        .attr(\"refY\", 0)\n        .attr(\"markerWidth\", 15)\n        .attr(\"markerHeight\", 15)\n        .attr(\"orient\", \"auto\")\n    .append(\"path\")\n        // draws a filled path (triangle) between three points\n        .attr(\"d\", \"M0,-5 L20,0 L0,5 z\")\n        .attr(\"fill\", '#999')\n        .attr(\"stroke\", 'none');\n\n    var g = svg.append(\"g\")\n        .attr(\"class\", \"everything\");\n\n    var link = g.append(\"g\")\n        .attr(\"class\", \"links_qNQqmuAW\")\n        .selectAll(\"line\")\n        .data(graph.links)\n        .enter().append(\"line\")\n        .attr(\"stroke-width\", function(d) { return d.width; })\n        .attr(\"stroke\", function(d) { return d.color; } );\n\n    if (directed)\n        link.attr(\"marker-end\", \"url(#end)\");\n\n    var node_g = g.append(\"g\")\n        .attr(\"class\", \"nodes_qNQqmuAW\")\n        .selectAll(\"circle\")\n        .data(graph.nodes)\n        .enter()\n        .append(\"g\"); \n\n    var node = node_g.append(\"circle\")\n        .attr('id', function(d) { return d.id; })\n        .attr(\"r\", function(d) { return d.size; })\n        .attr(\"fill\", function(d) { return d.color; })\n        .call(d3.drag()\n            .on(\"start\", dragstarted)\n            .on(\"drag\", dragged)\n            .on(\"end\", dragended));\n\n    var text = node_g.append(\"text\")\n    .attr(\"x\", [0, -10][0])\n    .attr(\"y\", [0, -10][1]+3)\n    .attr(\"id\", function(d) {return d.id; })\n    .attr(\"class\", \"label_qNQqmuAW\")\n    .text(function(d) { return d.id; });\n\n    node.append(\"title\")\n        .text(function(d) { return d.text; });\n\n    var zoom_handler = d3.zoom()\n        .on(\"zoom\", zoom_actions);\n    zoom_handler(svg);\n\n    // attach event handler for svg export\n    d3.select('#qNQqmuAW_svg_txt').on(\"click\", save_svg);\n\n    simulation\n        .nodes(graph.nodes)\n        .on(\"tick\", ticked);\n\n    simulation.force(\"link\")\n        .links(graph.links);\n\n    function ticked() {\n        link.attr(\"x1\", function(d) { return d.source.x; })\n            .attr(\"y1\", function(d) { return d.source.y; })\n            .attr(\"x2\", function(d) { return d.target.x; })\n            .attr(\"y2\", function(d) { return d.target.y; });\n\n         text.attr(\"transform\", transform);\n\n\n        text.attr(\"cx\", function(d) { return d.x; })\n            .attr(\"cy\", function(d) { return d.y; });\n\n        node.attr(\"cx\", function(d) { return d.x; })\n            .attr(\"cy\", function(d) { return d.y; });\n    }\n\n    function transform(d) {\n        return \"translate(\" + d.x + \",\" + d.y + \")\";\n    }\n\n    function dragstarted(d) {\n    if (!d3.event.active) simulation.alphaTarget(0.3).restart();\n    d.fx = d.x;\n    d.fy = d.y;\n    }\n\n    function zoom_actions(){\n        g.attr(\"transform\", d3.event.transform)\n    }    \n\n    function dragged(d) {\n    d.fx = d3.event.x;\n    d.fy = d3.event.y;\n    }\n\n    function dragended(d) {\n    if (!d3.event.active) simulation.alphaTarget(0);\n    d.fx = null;\n    d.fy = null;\n    }\n\n    function to_svg() {\n        // Returns a SVG representation of the current state of the visualisation\n\t    var svg  = document.getElementById('qNQqmuAW');\n\t    var xml = (new XMLSerializer).serializeToString(svg);\n        return xml;\n    }\n\n    function save_svg() {\n        var svg  = document.getElementById('qNQqmuAW');\n        var xml = (new XMLSerializer).serializeToString(svg);        \n        var blob = new Blob([xml], {type: 'text/xml'});\n        if(window.navigator.msSaveOrOpenBlob) {\n            window.navigator.msSaveBlob(blob, 'network.svg');\n        }\n        else{\n            var elem = window.document.createElement('a');\n            elem.href = window.URL.createObjectURL(blob);\n            elem.download = 'network.svg';\n            document.body.appendChild(elem);\n            elem.click();\n            document.body.removeChild(elem);\n        }\n    }\n});\n</script>",
      "text/plain": "<pathpy.classes.network.Network at 0x7f2b38f30e80>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#package for network visualization:\n",
    "import pathpy as pp\n",
    "\n",
    "#fetching information:\n",
    "t, node_info, edge_info = git2net.get_coediting_network(sqlite_db_file)\n",
    "\n",
    "#showing graph:\n",
    "pp.Network.from_temporal_network(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "This already gives us the idea that `Thomas Koch` is the centre figure of the community.\n",
    "\n",
    "Lets look at the work activity of the committers in the last 6 months, this gives us an idea who has recently been active. The following graph shows us which committer has edited which files from `01.05.2019` onwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<style>\n    text.label_iFDnGaIw {\n        text-anchor: middle;\n        font-size: 8px;\n        font-family: Arial, Helvetica, sans-serif;\n        fill: #999999;\n        stroke: #ffffff;\n        stroke-width: 0.1px;\n        opacity: 1.0;\n    }\n    .links_iFDnGaIw line {\n    stroke-opacity: 1.0;\n    }\n    .arrows svg:path {\n        stroke: #666;\n    }\n    .nodes_iFDnGaIw circle {\n    stroke: #fff;\n    stroke-width: 0.5px;\n    }\n</style>\n\n<svg width=\"400\" height=\"400\" id=\"iFDnGaIw\">        \n    <text x=\"20\" y=\"20\" font-family=\"sans-serif\" font-size=\"14px\" fill=\"#666\" style=\"cursor: pointer\"\n        id='iFDnGaIw_svg_txt'>[save svg]</text>\n</svg>\n\n<script charset=\"utf-8\" src=\"https://d3js.org/d3.v4.min.js\"></script>\n<script charset=\"utf-8\">\n\n// Load via requireJS if available (jupyter notebook environment)\ntry {\n    // Problem: require.config will raise an exception when called for the second time \n    require.config({\n        paths: {\n            //d3: \"https://d3js.org/d3.v4.min\"\n            d3: \"https://d3js.org/d3.v4.min.js\".replace(\".js\", \"\")\n        }\n    });\n    console.log(\"Detected requireJS\");\n}\ncatch(err){\n    // a reference error indicates that requireJS does not exist. \n    // other errors may occur due to multiple calls to config\n    if (err instanceof ReferenceError){\n        console.log(\"Detected no requireJS\");\n\n        // Helper function that waits for d3js to be loaded\n        require = function require(symbols, callback) {\n            var ms = 5;\n            window.setTimeout(function(t) {\n                if (window[symbols[0]])\n                    callback(window[symbols[0]]);\n                else \n                    window.setTimeout(arguments.callee, ms);\n            }, ms);\n        }\n    }\n}\n\n// wait until d3js has been loaded\nrequire([\"d3\"], function(d3) {\n    var svg = d3.select(\"#\"+\"iFDnGaIw\"), radius = 6, width = +svg.attr(\"width\"), height = +svg.attr(\"height\");\n\n    var color = d3.scaleOrdinal(d3.schemeCategory20);\n\n    var graph = {\"links\": [{\"source\": \"Thomas Koch\", \"target\": \"tlp.8\", \"color\": \"#999999\", \"width\": 0.5, \"weight\": 0.5}, {\"source\": \"Thomas Koch\", \"target\": \"changelog\", \"color\": \"#999999\", \"width\": 0.5, \"weight\": 0.9411764705882353}, {\"source\": \"Thomas Koch\", \"target\": \"tlp-func-base.in\", \"color\": \"#999999\", \"width\": 0.5, \"weight\": 0.9090909090909091}, {\"source\": \"Thomas Koch\", \"target\": \"tlp.service.in\", \"color\": \"#999999\", \"width\": 0.5, \"weight\": 1.0}, {\"source\": \"Thomas Koch\", \"target\": \"tlp-stat.in\", \"color\": \"#999999\", \"width\": 0.5, \"weight\": 0.8888888888888888}, {\"source\": \"Thomas Koch\", \"target\": \"n_35-tlp-func-batt\", \"color\": \"#999999\", \"width\": 0.5, \"weight\": 0.8}, {\"source\": \"Thomas Koch\", \"target\": \"default\", \"color\": \"#999999\", \"width\": 0.5, \"weight\": 1.0}, {\"source\": \"Thomas Koch\", \"target\": \"Makefile\", \"color\": \"#999999\", \"width\": 0.5, \"weight\": 0.8}, {\"source\": \"Thomas Koch\", \"target\": \"tlp.init\", \"color\": \"#999999\", \"width\": 0.5, \"weight\": 1.0}, {\"source\": \"Thomas Koch\", \"target\": \"n_10-tlp-func-cpu\", \"color\": \"#999999\", \"width\": 0.5, \"weight\": 1.0}, {\"source\": \"Thomas Koch\", \"target\": \"tlp-sleep.service.8\", \"color\": \"#999999\", \"width\": 0.5, \"weight\": 1.0}, {\"source\": \"Thomas Koch\", \"target\": \"tlp.service.8\", \"color\": \"#999999\", \"width\": 0.5, \"weight\": 1.0}, {\"source\": \"Thomas Koch\", \"target\": \"tlp.bash_completion\", \"color\": \"#999999\", \"width\": 0.5, \"weight\": 0.5}, {\"source\": \"Thomas Koch\", \"target\": \"tlp-sleep\", \"color\": \"#999999\", \"width\": 0.5, \"weight\": 1.0}, {\"source\": \"Thomas Koch\", \"target\": \"tlp-sleep.service.in\", \"color\": \"#999999\", \"width\": 0.5, \"weight\": 1.0}, {\"source\": \"Thomas Koch\", \"target\": \"tlp-func-stat\", \"color\": \"#999999\", \"width\": 0.5, \"weight\": 0.75}, {\"source\": \"Thomas Koch\", \"target\": \"tlp.in\", \"color\": \"#999999\", \"width\": 0.5, \"weight\": 1.0}, {\"source\": \"Thomas Koch\", \"target\": \"README.md\", \"color\": \"#999999\", \"width\": 0.5, \"weight\": 1.0}, {\"source\": \"linrunner\", \"target\": \"n_30-tlp-func-rf-sw\", \"color\": \"#999999\", \"width\": 0.5, \"weight\": 1.0}, {\"source\": \"linrunner\", \"target\": \"tlp.8\", \"color\": \"#999999\", \"width\": 0.5, \"weight\": 0.5}, {\"source\": \"linrunner\", \"target\": \"changelog\", \"color\": \"#999999\", \"width\": 0.5, \"weight\": 0.09090909090909091}, {\"source\": \"linrunner\", \"target\": \"Makefile\", \"color\": \"#999999\", \"width\": 0.5, \"weight\": 0.2}, {\"source\": \"linrunner\", \"target\": \"tlp-func-base.in\", \"color\": \"#999999\", \"width\": 0.5, \"weight\": 0.09090909090909091}, {\"source\": \"linrunner\", \"target\": \"n_40-tlp-func-bay\", \"color\": \"#999999\", \"width\": 0.5, \"weight\": 1.0}, {\"source\": \"linrunner\", \"target\": \"tlp-func-stat\", \"color\": \"#999999\", \"width\": 0.5, \"weight\": 0.25}, {\"source\": \"linrunner\", \"target\": \"tlp-stat.in\", \"color\": \"#999999\", \"width\": 0.5, \"weight\": 0.1111111111111111}, {\"source\": \"linrunner\", \"target\": \"n_35-tlp-func-batt\", \"color\": \"#999999\", \"width\": 0.5, \"weight\": 0.2}, {\"source\": \"linrunner\", \"target\": \"tlp.bash_completion\", \"color\": \"#999999\", \"width\": 0.5, \"weight\": 0.5}, {\"source\": \"linrunner\", \"target\": \"n_15-tlp-func-disk\", \"color\": \"#999999\", \"width\": 0.5, \"weight\": 1.0}], \"nodes\": [{\"id\": \"Thomas Koch\", \"text\": \"Thomas Koch\", \"color\": \"#73D2DE\", \"size\": 5.0}, {\"id\": \"tlp.8\", \"text\": \"tlp.8\", \"color\": \"#2E5EAA\", \"size\": 5.0}, {\"id\": \"changelog\", \"text\": \"changelog\", \"color\": \"#2E5EAA\", \"size\": 5.0}, {\"id\": \"tlp-func-base.in\", \"text\": \"tlp-func-base.in\", \"color\": \"#2E5EAA\", \"size\": 5.0}, {\"id\": \"tlp.service.in\", \"text\": \"tlp.service.in\", \"color\": \"#2E5EAA\", \"size\": 5.0}, {\"id\": \"tlp-stat.in\", \"text\": \"tlp-stat.in\", \"color\": \"#2E5EAA\", \"size\": 5.0}, {\"id\": \"n_35-tlp-func-batt\", \"text\": \"n_35-tlp-func-batt\", \"color\": \"#2E5EAA\", \"size\": 5.0}, {\"id\": \"default\", \"text\": \"default\", \"color\": \"#2E5EAA\", \"size\": 5.0}, {\"id\": \"Makefile\", \"text\": \"Makefile\", \"color\": \"#2E5EAA\", \"size\": 5.0}, {\"id\": \"tlp.init\", \"text\": \"tlp.init\", \"color\": \"#2E5EAA\", \"size\": 5.0}, {\"id\": \"n_10-tlp-func-cpu\", \"text\": \"n_10-tlp-func-cpu\", \"color\": \"#2E5EAA\", \"size\": 5.0}, {\"id\": \"tlp-sleep.service.8\", \"text\": \"tlp-sleep.service.8\", \"color\": \"#2E5EAA\", \"size\": 5.0}, {\"id\": \"tlp.service.8\", \"text\": \"tlp.service.8\", \"color\": \"#2E5EAA\", \"size\": 5.0}, {\"id\": \"tlp.bash_completion\", \"text\": \"tlp.bash_completion\", \"color\": \"#2E5EAA\", \"size\": 5.0}, {\"id\": \"tlp-sleep\", \"text\": \"tlp-sleep\", \"color\": \"#2E5EAA\", \"size\": 5.0}, {\"id\": \"tlp-sleep.service.in\", \"text\": \"tlp-sleep.service.in\", \"color\": \"#2E5EAA\", \"size\": 5.0}, {\"id\": \"tlp-func-stat\", \"text\": \"tlp-func-stat\", \"color\": \"#2E5EAA\", \"size\": 5.0}, {\"id\": \"tlp.in\", \"text\": \"tlp.in\", \"color\": \"#2E5EAA\", \"size\": 5.0}, {\"id\": \"README.md\", \"text\": \"README.md\", \"color\": \"#2E5EAA\", \"size\": 5.0}, {\"id\": \"linrunner\", \"text\": \"linrunner\", \"color\": \"#73D2DE\", \"size\": 5.0}, {\"id\": \"n_30-tlp-func-rf-sw\", \"text\": \"n_30-tlp-func-rf-sw\", \"color\": \"#2E5EAA\", \"size\": 5.0}, {\"id\": \"n_40-tlp-func-bay\", \"text\": \"n_40-tlp-func-bay\", \"color\": \"#2E5EAA\", \"size\": 5.0}, {\"id\": \"n_15-tlp-func-disk\", \"text\": \"n_15-tlp-func-disk\", \"color\": \"#2E5EAA\", \"size\": 5.0}]};\n    var directed = true;\n\n    var simulation = d3.forceSimulation()        \n       .force(\"link\", d3.forceLink().id(function(d) { return d.id; }).strength(function(d){return d.weight;}))\n        .force(\"charge\", d3.forceManyBody().strength(-20).distanceMax(400))\n       .force(\"repelForce\", d3.forceManyBody().strength(-200).distanceMax(100))\n       .force(\"center\", d3.forceCenter(400 / 2, 400 / 2))\n       .alphaTarget(0.0);\n\n    svg.append(\"defs\").selectAll(\"marker\")\n        .data([\"end\"])\n    .enter().append(\"marker\")\n        .attr(\"id\", String)\n        .attr(\"viewBox\", \"0 -5 20 20\")\n        .attr(\"refX\", 34)\n        .attr(\"refY\", 0)\n        .attr(\"markerWidth\", 15)\n        .attr(\"markerHeight\", 15)\n        .attr(\"orient\", \"auto\")\n    .append(\"path\")\n        // draws a filled path (triangle) between three points\n        .attr(\"d\", \"M0,-5 L20,0 L0,5 z\")\n        .attr(\"fill\", '#999')\n        .attr(\"stroke\", 'none');\n\n    var g = svg.append(\"g\")\n        .attr(\"class\", \"everything\");\n\n    var link = g.append(\"g\")\n        .attr(\"class\", \"links_iFDnGaIw\")\n        .selectAll(\"line\")\n        .data(graph.links)\n        .enter().append(\"line\")\n        .attr(\"stroke-width\", function(d) { return d.width; })\n        .attr(\"stroke\", function(d) { return d.color; } );\n\n    if (directed)\n        link.attr(\"marker-end\", \"url(#end)\");\n\n    var node_g = g.append(\"g\")\n        .attr(\"class\", \"nodes_iFDnGaIw\")\n        .selectAll(\"circle\")\n        .data(graph.nodes)\n        .enter()\n        .append(\"g\"); \n\n    var node = node_g.append(\"circle\")\n        .attr('id', function(d) { return d.id; })\n        .attr(\"r\", function(d) { return d.size; })\n        .attr(\"fill\", function(d) { return d.color; })\n        .call(d3.drag()\n            .on(\"start\", dragstarted)\n            .on(\"drag\", dragged)\n            .on(\"end\", dragended));\n\n    var text = node_g.append(\"text\")\n    .attr(\"x\", [0, -10][0])\n    .attr(\"y\", [0, -10][1]+3)\n    .attr(\"id\", function(d) {return d.id; })\n    .attr(\"class\", \"label_iFDnGaIw\")\n    .text(function(d) { return d.id; });\n\n    node.append(\"title\")\n        .text(function(d) { return d.text; });\n\n    var zoom_handler = d3.zoom()\n        .on(\"zoom\", zoom_actions);\n    zoom_handler(svg);\n\n    // attach event handler for svg export\n    d3.select('#iFDnGaIw_svg_txt').on(\"click\", save_svg);\n\n    simulation\n        .nodes(graph.nodes)\n        .on(\"tick\", ticked);\n\n    simulation.force(\"link\")\n        .links(graph.links);\n\n    function ticked() {\n        link.attr(\"x1\", function(d) { return d.source.x; })\n            .attr(\"y1\", function(d) { return d.source.y; })\n            .attr(\"x2\", function(d) { return d.target.x; })\n            .attr(\"y2\", function(d) { return d.target.y; });\n\n         text.attr(\"transform\", transform);\n\n\n        text.attr(\"cx\", function(d) { return d.x; })\n            .attr(\"cy\", function(d) { return d.y; });\n\n        node.attr(\"cx\", function(d) { return d.x; })\n            .attr(\"cy\", function(d) { return d.y; });\n    }\n\n    function transform(d) {\n        return \"translate(\" + d.x + \",\" + d.y + \")\";\n    }\n\n    function dragstarted(d) {\n    if (!d3.event.active) simulation.alphaTarget(0.3).restart();\n    d.fx = d.x;\n    d.fy = d.y;\n    }\n\n    function zoom_actions(){\n        g.attr(\"transform\", d3.event.transform)\n    }    \n\n    function dragged(d) {\n    d.fx = d3.event.x;\n    d.fy = d3.event.y;\n    }\n\n    function dragended(d) {\n    if (!d3.event.active) simulation.alphaTarget(0);\n    d.fx = null;\n    d.fy = null;\n    }\n\n    function to_svg() {\n        // Returns a SVG representation of the current state of the visualisation\n\t    var svg  = document.getElementById('iFDnGaIw');\n\t    var xml = (new XMLSerializer).serializeToString(svg);\n        return xml;\n    }\n\n    function save_svg() {\n        var svg  = document.getElementById('iFDnGaIw');\n        var xml = (new XMLSerializer).serializeToString(svg);        \n        var blob = new Blob([xml], {type: 'text/xml'});\n        if(window.navigator.msSaveOrOpenBlob) {\n            window.navigator.msSaveBlob(blob, 'network.svg');\n        }\n        else{\n            var elem = window.document.createElement('a');\n            elem.href = window.URL.createObjectURL(blob);\n            elem.download = 'network.svg';\n            document.body.appendChild(elem);\n            elem.click();\n            document.body.removeChild(elem);\n        }\n    }\n});\n</script>",
      "text/plain": "<IPython.core.display.HTML object>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "time_from = datetime(2019, 5, 1)\n",
    "t, node_info, edge_info = git2net.get_bipartite_network(sqlite_db_file, time_from=time_from)\n",
    "\n",
    "n = pp.Network.from_temporal_network(t)\n",
    "colour_map = {'author': '#73D2DE', 'file': '#2E5EAA'}\n",
    "node_colour = {node: colour_map[node_info['class'][node]] for node in n.nodes}\n",
    "pp.visualisation.plot(n, node_color=node_colour)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Again, `Thomas Koch` seems not only to be the centre figure of the community, but also one of the only few with recent activity, doing most of the work himself.\n",
    "\n",
    "## Sqlite queries\n",
    "\n",
    "Lets begin to analyse the sqlite database, by connecting to the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "#connect to database file:\n",
    "con = sqlite3.connect(sqlite_db_file)\n",
    "c = con.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Since we only want to look at the edit type `replacement` in the `edits`-table, lets create a new table `df2` with the columns that we need for linear regression, namely:\n",
    "\n",
    "1. the distinct `hash` commit value\n",
    "2. the `committer_name`\n",
    "3. the `committer_date`\n",
    "4. the parent commit hash `original_commit_deletion`\n",
    "5. and the producitivity measure `levenshtein_dist`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<sqlite3.Cursor at 0x7f2b8479c030>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create a new table with the desired column names:\n",
    "query0=\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS df2(\n",
    "    hash TEXT PRIMARY KEY,\n",
    "    committer_name TEXT,\n",
    "    committer_date TEXT,\n",
    "    original_commit_deletion TEXT,\n",
    "    levenshtein_dist INTEGER);\n",
    "\"\"\"\n",
    "\n",
    "#execute the queries on the database:\n",
    "c.execute(query0)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Following task 02, lets select only rows of table `edits`, where `edit_type='replacement'` and merge this with table `commits` along the primary key `hash`. This gives us a joined table of `edits` and `commits`, but with hashes occuring multiple times, each with different `levenshtein_dist`.\n",
    "\n",
    "Therefore we group this query by unique hashes, taking the sum of the `levenshtein dist` of same hashes, because we want to know the total `levenshtein_dist` for each commit and not for every single edit.\n",
    "\n",
    "Finally, we insert this into the new table `df2` to avoid key words (e.g. `SUM(levenshtein_dist)`) as column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<sqlite3.Cursor at 0x7f2b8479c030>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query1=\"\"\"\n",
    "    INSERT INTO df2(hash, committer_name, committer_date, original_commit_deletion, levenshtein_dist)\n",
    "    SELECT hash, committer_name, committer_date, original_commit_deletion, \"SUM(levenshtein_dist)\"\n",
    "    FROM\n",
    "    (\n",
    "    SELECT \n",
    "    hash,\n",
    "    committer_name,\n",
    "    committer_date,\n",
    "    original_commit_deletion,\n",
    "    SUM(levenshtein_dist)\n",
    "    FROM(SELECT * FROM edits INNER JOIN commits ON commits.hash = edits.commit_hash WHERE edit_type = 'replacement')\n",
    "    GROUP BY hash\n",
    "    );\n",
    "\"\"\"\n",
    "\n",
    "c.execute(query1)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "We can count the distinct `committer_names`, this gives us an idea of the size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "The number of committers we are dealing with is 11\n"
    }
   ],
   "source": [
    "#count the distinct committer names:\n",
    "query1_1=\"\"\"\n",
    "    SELECT count(DISTINCT committer_name) FROM df2;\n",
    "\"\"\"\n",
    "\n",
    "c.execute(query1_1)\n",
    "\n",
    "#store amount of distinct committer names as int...\n",
    "number_committers = c.fetchone()[0]\n",
    "\n",
    "print('The number of committers we are dealing with is', number_committers)\n",
    "\n",
    "#save the queries:\n",
    "con.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lets take a look of the committers names with the following query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select the distinct committer names\n",
    "query1_2=\"\"\"\n",
    "     SELECT DISTINCT committer_name FROM df2;\n",
    "\"\"\"\n",
    "\n",
    "c.execute(query1_2)\n",
    "\n",
    "#store names of committers in list:\n",
    "committer_list = c.fetchall()\n",
    "\n",
    "#save the queries:\n",
    "con.commit()\n",
    "\n",
    "#close the database to make it accessible for others:\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "The committers names are:\n\n"
    },
    {
     "data": {
      "text/plain": "['Thomas Koch',\n 'GitHub',\n 'SammysHP',\n 'Andr√© Erdmann',\n 'Aaditya Bagga',\n 'Timofey Titovets',\n 'Connor Prussin',\n 'TK',\n 'Kai-Heng Feng',\n 'Maxime Gauduin',\n 'Qiang Yu']"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#turn list of string-tuples into list of single-strings:\n",
    "committers = [i[0] for i in committer_list]\n",
    "\n",
    "print('The committers names are:\\n')\n",
    "\n",
    "committers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "We want to find a productivity measure that suits each commit. The idea is to find the next commit, that an author did, and calculate the `levensthein_dist` for that specific time intervall between the two commits. Therefore, we first need to sort the `df2`-table by ascending `commit_dates`.\n",
    "\n",
    "We call this new table `df3`. We do this by executing our sorting query on `df2` and transfering everything to `df3`. By doing this procedure we avoid table errors and unclean workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create new empty table:\n",
    "query01=\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS df3(\n",
    "    hash TEXT PRIMARY KEY,\n",
    "    committer_name TEXT,\n",
    "    committer_date TEXT,\n",
    "    original_commit_deletion TEXT,\n",
    "    levenshtein_dist INTEGER);\n",
    "\"\"\"\n",
    "\n",
    "#sort committer_date ascending:\n",
    "query2=\"\"\"\n",
    "    INSERT INTO df3(hash, committer_name, committer_date, original_commit_deletion, levenshtein_dist)\n",
    "    SELECT *\n",
    "    FROM\n",
    "    (\n",
    "    SELECT * \n",
    "    FROM df2 \n",
    "    ORDER BY committer_date ASC\n",
    "    );\n",
    "\"\"\"\n",
    "\n",
    "#delete table df2:\n",
    "query3=\"\"\"\n",
    "    DROP TABLE df2;\n",
    "\"\"\"\n",
    "\n",
    "con = sqlite3.connect(sqlite_db_file)\n",
    "c = con.cursor()\n",
    "\n",
    "#execute the queries:\n",
    "c.execute(query01)\n",
    "c.execute(query2)\n",
    "c.execute(query3)\n",
    "#result: df3\n",
    "\n",
    "#close connection:\n",
    "con.commit()\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Numpy array\n",
    "\n",
    "Now we have a clean table with ascending `committer_dates`. We can transform this into a numpy array for faster and more precise calculations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "The first row of the np array: \n ['4b6204169763d76567c846528be692afd30c38e4' 'Thomas Koch'\n '2010-01-22 18:16:44' '624dc813bd780af99e0d73e31fd2de9f3cc6c9ad' 4]\n"
    }
   ],
   "source": [
    "#reopen connection:\n",
    "con = sqlite3.connect(sqlite_db_file)\n",
    "\n",
    "#output df2 as a pandas dataframe:\n",
    "df = pd.read_sql_query(\"SELECT * FROM df3;\", con=con)\n",
    "\n",
    "#turn Pandas dataframe into numpy array:\n",
    "df = df.values\n",
    "\n",
    "print('The first row of the np array: \\n', df[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Productivity measure\n",
    "\n",
    "Since we will need to extend the array for the extra productivity value that we will calculate, we introduce a small function that appends further columns to our array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#function for adding a full-length '0'-column to the dataframe:\n",
    "def add_column(df):\n",
    "    new_column = np.array(np.zeros(df.shape[0]))\n",
    "    return np.column_stack((df, new_column))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Shape of array: (942, 6)\n"
    }
   ],
   "source": [
    "#add column:\n",
    "df = add_column(df)\n",
    "print('Shape of array:', df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In order to find out the total time between two commits of each author, we must turn the `committer_date` (string) into a readible numeric object. We can do this by stripping the time variables and returning a `datetime`-object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "2010-01-22 18:16:44\n<class 'datetime.datetime'>\n"
    }
   ],
   "source": [
    "#make date stamps readible, by turning them into a datetime-object:\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "def date_to_int(date_str):\n",
    "    return datetime.strptime(date_str, '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "print(date_to_int(df[0][2]))\n",
    "print(type(date_to_int(df[0][2])))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Since the table is sorted by ascending `committer_dates`, we can simply run a loop and check for the next commit that a certain author delivered. We then calculate the amount of time (in seconds) between the two commits; we save the value in the newly created column.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(df.shape[0]-1):\n",
    "    #grab the author and his commit_time:\n",
    "    author = df[i][1]\n",
    "    commit_time = df[i][2]\n",
    "    for j in range(i+1,df.shape[0]):\n",
    "        #search for the his next commit:\n",
    "        author_next = df[j][1]\n",
    "        commit_time_next = df[j][2]\n",
    "        #if there is a \"next commit\", calculate the commit_time differences (in total seconds) and save the value in the last column:\n",
    "        if author == author_next:\n",
    "                #calulate the total time (in seconds) between the 2 commits:\n",
    "                df[j][5] = (date_to_int(commit_time_next) - date_to_int(commit_time)).total_seconds()\n",
    "                break\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Shape of array: (942, 7)\n"
    }
   ],
   "source": [
    "#add empty 6th column:\n",
    "df = add_column(df)\n",
    "print('Shape of array:', df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "If someone has a `levensthein_dist` of 1000, but took 3 months to commit it, he was not very productive.\n",
    "Whereas someone with that same `levensthein_dist`, who commited in 3 days, is far more productive. \n",
    "\n",
    "Therefore, we define the productivity measure for each authors commit to be the `levensthein_dist` per second:\n",
    "\n",
    "$$\\text{levensthein_dist/second} \\quad = \\quad \\frac{\\text{levensthein_dist}}{\\text{time_between_two_commits}}$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(df.shape[0]):\n",
    "    #if time_between_commits = 0, then set productivity to 0\n",
    "    if df[i][5] == 0:\n",
    "        df[i][6] = 0\n",
    "    #else, calculate productivity measure and store in 6th column\n",
    "    else:     \n",
    "        df[i][6] = df[i][4] / df[i][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Shape of array: (942, 8)\n"
    }
   ],
   "source": [
    "#add empty 7th column:\n",
    "df = add_column(df)\n",
    "print('Shape of array:', df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Collaboration variable\n",
    "\n",
    "Following task 03, we want to construct a variable defining if the author collaborated or not for his commit, i.e. whether or not he is editing someone elses commit.\n",
    "\n",
    "We set a boolean variable\n",
    "- '1' if the authors parents hash is another authors commit hash (collaborated: True)\n",
    "- '0' if this is not the case (collaborated: False)\n",
    "\n",
    "and store this in the newly created column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(df.shape[0]):\n",
    "    parent_hash = df[i][3]\n",
    "    #set orig_author to default 'nobody'\n",
    "    #(important for genesis commit hashes)\n",
    "    orig_author = 'nobody'\n",
    "    for j in range(i):\n",
    "        #check whether any other prior hash is from the same author\n",
    "        if df[j][0] == parent_hash:\n",
    "            #update orig_author with real author name:\n",
    "            orig_author = df[j][1]\n",
    "            #if original hash found, then break:\n",
    "            break\n",
    "    #set a place holder in the 7th column:\n",
    "    df[i][7] = 'no parent_hash'\n",
    "    #if author collaborated, save '1', if not, save '0':\n",
    "    #only if orig_author was another author, did the author collaborate:\n",
    "    if orig_author != df[i][1] and orig_author != 'nobody':\n",
    "        df[i][7] = 1\n",
    "    else:\n",
    "        df[i][7] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Now check the `df`-variable. We know have all the data in the columns needed for linear regression!\n",
    "\n",
    "Specifically, we only need\n",
    "\n",
    "1. the `committer_name`, capturing the identity of the developer. Each developer is obviously differently skilled, so we want to capture this effect in a specified variable when doing linear regression\n",
    "\n",
    "2. the `levenshtein_dist/second`, measuring the productivity of every authors commit\n",
    "\n",
    "3. the `collab_bool`, telling us whether or not the author \"collaborated\"\n",
    "\n",
    "We delete every other column of our array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Shape of array: (942, 3)\n"
    }
   ],
   "source": [
    "#delete commit_hash:\n",
    "df = np.delete(df,0,1)\n",
    "#delete commit_time:\n",
    "df = np.delete(df,1,1)\n",
    "#delete parent_hash:\n",
    "df = np.delete(df,1,1)\n",
    "#delete levensthein_dist:\n",
    "df = np.delete(df,1,1)\n",
    "#delete time_between_commits:\n",
    "df = np.delete(df,1,1)\n",
    "\n",
    "print('Shape of array:', df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Its most convenient to perform (multiple) linear regression using `statsmodels`. This works best if we use a `pandas` dataframe, since we can call columns simply by their names. So in the following, we convert our `numpy` array into a 3-column `pandas` dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "(942, 3)\n  committer_names  productivity_per_second collab_bool\n0     Thomas Koch                 0.000000           0\n1     Thomas Koch                 0.000099           0\n2     Thomas Koch                 0.007752           0\n"
    }
   ],
   "source": [
    "#insert first column of df as 'committer_names':\n",
    "pdf = pd.DataFrame({'committer_names': df[:,0]})\n",
    "#insert second column of df as 'productivity_per_second':\n",
    "pdf['productivity_per_second'] = df[:,1]\n",
    "#we must account for the lack of pandas' ability to handle scientific numeric notation (e.g. e^-5) by specifically turning them into numeric values:\n",
    "pdf['productivity_per_second'] = pd.to_numeric(pdf['productivity_per_second'])\n",
    "#insert third column of df as 'collab_bool':\n",
    "pdf['collab_bool'] = df[:,2]\n",
    "\n",
    "print(pdf.shape)\n",
    "print(pdf.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Multiple linear regression\n",
    "\n",
    "Finally we can run linear regression!\n",
    "\n",
    "We must keep in mind, that the `committer_names` are `strings` and for each `committer_name`, we want to have a variable that captures the developers identity. Since they are 'strings', it would make them `categorical` variables. We can captures all of the different `committer_names` by introducing `dummy` variables. \n",
    "\n",
    "Thankfully, `statsmodels` sports an useful feature for automatically dealing with turning these `categorical` into `dummy` variables. \n",
    "We can neglect any common influence of one certain `committer_name` with another.\n",
    "\n",
    "Our multiple linear regression therefore consists of $$\\beta_{\\text{col}} \\cdot \\text{collab_bool} \\quad + \\quad \\textbf{beta}^{\\text{T}} \\cdot \\textbf{committer_names} \\quad = \\quad  \\text{productivity_per_second},$$\n",
    "where $\\textbf{beta} = (\\beta_0, \\ldots, \\beta_{\\text{number_committers}+1})$, ($\\beta_0$ accounts for the intercept) \n",
    "\n",
    "$\\textbf{committer_names} = (\\mathbb{1}, \\text{committer_names}[0], \\ldots, \\text{committer_names}[\\text{number_committers}])$\n",
    "\n",
    "By investigating the value of the estimated $\\beta_{\\text{col}}$, we can explain the linear effect of collaborating on the productivity. This may allow us to discuss the Ringelmann effect when comparing different group sizes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "OLS Regression Results                              \n===================================================================================\nDep. Variable:     productivity_per_second   R-squared:                       0.008\nModel:                                 OLS   Adj. R-squared:                 -0.004\nMethod:                      Least Squares   F-statistic:                    0.6453\nDate:                     Sat, 16 Nov 2019   Prob (F-statistic):              0.790\nTime:                             22:15:48   Log-Likelihood:                -2825.6\nNo. Observations:                      942   AIC:                             5675.\nDf Residuals:                          930   BIC:                             5733.\nDf Model:                               11                                         \nCovariance Type:                 nonrobust                                         \n=======================================================================================================\n                                          coef    std err          t      P>|t|      [0.025      0.975]\n-------------------------------------------------------------------------------------------------------\nIntercept                            8.236e-06      3.457   2.38e-06      1.000      -6.785       6.785\ncollab_bool[T.1]                       -1.0798      1.486     -0.727      0.468      -3.995       1.836\ncommitter_names[T.Andr√© Erdmann]        9.6649      4.945      1.954      0.051      -0.040      19.370\ncommitter_names[T.Connor Prussin]       1.0798      6.169      0.175      0.861     -11.028      13.187\ncommitter_names[T.GitHub]               0.8098      4.378      0.185      0.853      -7.783       9.402\ncommitter_names[T.Kai-Heng Feng]        1.0798      6.169      0.175      0.861     -11.028      13.187\ncommitter_names[T.Maxime Gauduin]       1.2096      5.110      0.237      0.813      -8.819      11.238\ncommitter_names[T.Qiang Yu]             1.0798      6.169      0.175      0.861     -11.028      13.187\ncommitter_names[T.SammysHP]             1.0798      6.169      0.175      0.861     -11.028      13.187\ncommitter_names[T.TK]                   0.7245      4.572      0.158      0.874      -8.248       9.696\ncommitter_names[T.Thomas Koch]          0.3041      3.461      0.088      0.930      -6.488       7.096\ncommitter_names[T.Timofey Titovets]     1.0798      6.169      0.175      0.861     -11.028      13.187\n==============================================================================\nOmnibus:                     2354.421   Durbin-Watson:                   1.337\nProb(Omnibus):                  0.000   Jarque-Bera (JB):         18144767.642\nSkew:                          25.023   Prob(JB):                         0.00\nKurtosis:                     681.073   Cond. No.                         112.\n==============================================================================\n\nWarnings:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
    }
   ],
   "source": [
    "import statsmodels.formula.api as sm\n",
    "\n",
    "# format: y ~ x + committer_names:\n",
    "model = sm.ols(formula='productivity_per_second ~ collab_bool + committer_names', data = pdf)\n",
    "#train model:\n",
    "fit = model.fit()\n",
    "\n",
    "#print results:\n",
    "print(fit.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Summary of `TLP` repo\n",
    "\n",
    "We can see all the valuable information in the summary. Aside of giving us the calculated intercept and the various slope coefficients, we may also interpret other useful parameters.\n",
    "\n",
    "The negative `collab_bool` value could suggest a negative impact on productivity if committers are collaborating, but: we defined this value to simply be a `True` or `False` value whether or not the committer is working on a commit of another author. This is simply not strong enough for explaining real collaboration! It would need at least 2 back-and-fourth commits of two authors to be able to call it a collaboration. \n",
    "Furthermore, the estimated `collab_bool` value of $-1.0798$ does not seem robust, since we know from the visualizations that basically only Thomas Koch did all the work, no real collaboration can be found in the data. Any other of the few authors editing files of Thomas Koch is distorting this value, since they are not way as productive as lone wolf Thomas Koch is. This negative `collab_bool` is not robust enough to support a decline of productivity for collaborating team members. Significantly large confidence intervalls further nourish this hypothesis. \n",
    "\n",
    "In addition, we observe fairly high p-values in the summary for almost all coefficients, so we cannot reject the hypothesis, that there is no linear relationship in the data, thus we may assume that there is no linear relationship at all!\n",
    "\n",
    "In order to even prove a linear relationship in the data, the assumption of normally distributed residuals must hold. However, we need to check the normality of the residuals to test for a linear relationship. We can immediately see that $\\text{Prob(Omnibus)}=0$ and $\\text{Prob(JB)}=0$, this allows to confidently reject the hypothesis, that the residuals are normally distributed. Summarising the analysis, we can assume that there is no linear relationship in the data, thus we cannot properly discuss the negative `collab_bool` in respect to the Ringelmann effect."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Predictions\n",
    "\n",
    "We can now construct fictive predictions, by creating a dataframe with the corresponding exog values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "The predictions of the first 5 samples in the dataset [Thomas Koch, False]:\n 0    0.304082\n1    0.304082\n2    0.304082\n3    0.304082\n4    0.304082\ndtype: float64\n\n\nThe predictions of [TK, True] (pdf[206]): -0.35528465888192673\n\n\nThe predictions of [TK, False] (pdf[207]): 0.7244880110708445\n\n\nThe predictions of [Connor Prussin, True] (pdf[324]): 1.031628700921914e-14\n"
    }
   ],
   "source": [
    "#In Sample Predictions:\n",
    "print('The predictions of the first 5 samples in the dataset [Thomas Koch, False]:\\n', fit.predict(pdf).head())\n",
    "print('\\n')\n",
    "print('The predictions of [TK, True] (pdf[206]):',fit.predict(pdf)[206])\n",
    "print('\\n')\n",
    "print('The predictions of [TK, False] (pdf[207]):',fit.predict(pdf)[207])\n",
    "print('\\n')\n",
    "print('The predictions of [Connor Prussin, True] (pdf[324]):',fit.predict(pdf)[324])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "-0.7756908380888659"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Construct prediction for: [Thomas Koch, True]\n",
    "X_pred = pd.DataFrame({'committer_names': 'Thomas Koch'}, index=[0])\n",
    "X_pred['collab_bool'] = 1\n",
    "\n",
    "pred = fit.predict(X_pred)\n",
    "pred[0]"
   ]
  }
 ]
}